{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro\n",
    "Quick & dirty experiment to evaluate `natural language -> CLI command` conversion using a LLM with a simple API doc context instead of zero/few-shot examples. <br/>\n",
    "Nerfstudio provides a simple API that allows for a simplified end-to-end process of creating, training, and testing NeRFs. This is an attempt to take a text description of a task and convert it to CLI commands for handling the sequential steps of data processing, training and rendering. <br/>\n",
    "\n",
    "We evaluate some simple prompting techniques (zero-shot CoT, self-criticism) and culminate in a mock interactive session where a user can reject a response at any step and/or provide feedback to request a correction.\n",
    "\n",
    "Other notes:\n",
    "- limited, high-level subset of NeRFStudio CLI docs without detailed instructions using gpt-3.5-turbo.\n",
    "- doesn't leverage capabilities like gpt message roles, sophisticated prompting etc.\n",
    "- Performance is evaluated only on provided context. <br/>\n",
    "  - Doing this provides guidance on reducing hallucinations. On a larger scale, we could imagine working with multiple versions of a constantly changing API spec. Keeping it context-focused can aid this when using something like retrieval-augmented generation for larger or more context-focused docs.\n",
    "  - For eg, instruct-nerf2nerf requires first training a regular nerfacto scene and using those checkpoints. However, this info isn't in-context, so we don't care about this.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from IPython.display import Markdown, display, clear_output\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('context.txt', 'r') as file:\n",
    "    context = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A basic NeRF request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = \"Given a folder of images of a fire hydrant in './fireHydrant', make a NERF model of it and render it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\" You are a programmer specialized in producing NeRFs. You're given an API specification for the NerfStudio CLI, which can be used to make NERFs for several scenarios. \n",
    "API specification: ```{context}```\n",
    "Based on this API specification, write a CLI command that will produce a NERF for the following scenario: {scenario}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full prompt with context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " You are a programmer specialized in producing NeRFs. You're given an API specification for the NerfStudio CLI, which can be used to make NERFs for several scenarios. \n",
       "API specification: ```The following CLI modules is available for use:\n",
       "- ns-process-data: Process images into a nerfstudio dataset. Scales images to a specified size and calculates the camera poses for each image using COLMAP.\n",
       "  usage: ns-process-data [-h]\n",
       "                       {images,video,polycam,metashape,realitycapture,record3d}\n",
       "  ns-process-data images [-h] --data PATH --output-dir PATH\n",
       "                       [--verbose | --no-verbose]\n",
       "                       [--camera-type {perspective,fisheye,equirectangular}]\n",
       "                       [--matching-method {exhaustive,sequential,vocab_tree}]\n",
       "                       [--sfm-tool {any,colmap,hloc}]\n",
       "                       [--refine-pixsfm | --no-refine-pixsfm]\n",
       "                       [--feature-type \n",
       "{any,sift,superpoint,superpoint_aachen,superpoint_max,superpoint_inloc,r2d2,d2\n",
       "net-ss,sosnet,disk}]\n",
       "                       [--matcher-type \n",
       "{any,NN,superglue,superglue-fast,NN-superpoint,NN-ratio,NN-mutual,adalam}]\n",
       "                       [--num-downscales INT]\n",
       "                       [--skip-colmap | --no-skip-colmap]\n",
       "                       [--skip-image-processing | --no-skip-image-processing]\n",
       "                       [--colmap-model-path PATH] [--colmap-cmd STR]\n",
       "                       [--images-per-equirect {8,14}]\n",
       "                       [--crop-factor FLOAT FLOAT FLOAT FLOAT]\n",
       "                       [--crop-bottom FLOAT] [--gpu | --no-gpu]\n",
       "                       [--use-sfm-depth | --no-use-sfm-depth]\n",
       "                       [--include-depth-debug | --no-include-depth-debug]\n",
       "\n",
       "- ns-train: Primary interface for training a NeRF model\n",
       "  usage: ns-train {method} [method args] {dataparser} [dataparser args]\n",
       "    \n",
       "    To learn about the available methods:\n",
       "    ns-train --help\n",
       "    To learn about a methods parameters:\n",
       "\n",
       "    ns-train {method} --help\n",
       "    By default the nerfstudio dataparser is used. If you would like to use a different dataparser it can be specified after all of the method arguments. For a list of dataparser options:\n",
       "    ns-train {method} {dataparser} --help\n",
       "\n",
       "    methods:\n",
       "        - vanilla-nerf: The original NERF implementation. Generally not recommended for most applications.\n",
       "        - nerfacto: We created the nerfacto model to act as our default for real data captures of static scenes. The model is not existing published work, but rather a combination of many published methods that we have found work well for real data.\n",
       "        - instant-ngp: Many of the main contributions of Instant-NGP are built into our Nerfacto method, so for real-world scenes, we recommend using the Nerfacto model.\n",
       "        - in2n: instruct-NeRF2NeRF is a method for editing NeRF scenes with text-instructions. Given a NeRF of a scene and the collection of images used to reconstruct it, the method uses an image-conditioned diffusion model (InstructPix2Pix) to iteratively edit the input images while optimizing the underlying scene, resulting in an optimized 3D scene that respects the edit instruction. The paper demonstrates that their method is able to edit large-scale, real-world scenes, and is able to accomplish more realistic, targeted edits than prior work.\n",
       "        - lerf: LERF enables pixel-aligned queries of the distilled 3D CLIP embeddings without relying on region proposals, masks, or fine-tuning, supporting long-tail open-vocabulary queries hierarchically across the volume.\n",
       "        - tetra-nerf: The input to Tetra-NeRF is a point cloud which is triangulated to get a set of tetrahedra used to represent the radiance field. Rays are sampled, and the field is queried. The barycentric interpolation is used to interpolate tetrahedra vertices, and the resulting features are passed to a shallow MLP to get the density and colours for volumetric rendering.\n",
       "\n",
       "- ns-viewer: Load a checkpoint and start the viewer.\n",
       "  usage: ns-viewer [-h] --load-config PATH [--viewer.relative-log-filename STR]\n",
       "    [--viewer.websocket-port {None}|INT]\n",
       "    [--viewer.websocket-port-default INT]\n",
       "    [--viewer.websocket-host STR]\n",
       "    [--viewer.max-num-display-images INT]\n",
       "    [--viewer.quit-on-train-completion | \n",
       "    --viewer.no-quit-on-train-completion]\n",
       "                [--viewer.image-format {jpeg,png}]\n",
       "                [--viewer.jpeg-quality INT] [--vis {viewer,viewer_beta}]\n",
       "\n",
       "The general workflow for training a model is as follows:\n",
       "- Process your images into a nerfstudio dataset using ns-process-data\n",
       "- Train a model using ns-train. Choose the NERF model to use carefully based on the needs and requirements.\n",
       "- View the results using ns-viewer```\n",
       "Based on this API specification, write a CLI command that will produce a NERF for the following scenario: Given a folder of images of a fire hydrant in './fireHydrant', make a NERF model of it and render it."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(prompt.format(context=context, scenario=scenario)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To produce a NERF model for the given scenario, you can use the following CLI command:\n",
       "\n",
       "```\n",
       "ns-process-data images --data ./fireHydrant --output-dir ./fireHydrantDataset\n",
       "ns-train nerfacto --data ./fireHydrantDataset\n",
       "ns-viewer --load-config ./fireHydrantDataset/config.txt\n",
       "```\n",
       "\n",
       "Explanation:\n",
       "1. `ns-process-data images --data ./fireHydrant --output-dir ./fireHydrantDataset`: This command processes the images in the `./fireHydrant` folder and generates a nerfstudio dataset in the `./fireHydrantDataset` directory. It automatically scales the images and calculates camera poses using COLMAP.\n",
       "2. `ns-train nerfacto --data ./fireHydrantDataset`: This command trains a NERF model using the `nerfacto` method on the generated dataset in the `./fireHydrantDataset` directory.\n",
       "3. `ns-viewer --load-config ./fireHydrantDataset/config.txt`: This command loads the trained model checkpoint from the `./fireHydrantDataset` directory and starts the viewer to render the NERF model.\n",
       "\n",
       "Make sure to replace `./fireHydrant` with the actual path to the folder containing the fire hydrant images."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion(prompt.format(context=context, scenario=scenario))\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works fine, let's try a more complex scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To produce a NERF model for the given scenario and enable editing based on text-instructions, you can use the following CLI command:\n",
       "\n",
       "```\n",
       "ns-process-data images --data ./fireHydrant --output-dir ./fireHydrantDataset --camera-type perspective\n",
       "```\n",
       "\n",
       "This command will process the images in the './fireHydrant' folder and create a nerfstudio dataset in the './fireHydrantDataset' directory. The camera type is set to 'perspective' as it is the most common camera type for general images.\n",
       "\n",
       "After processing the data, you can train a NERF model using the 'ns-train' command. Since you want to enable editing based on text-instructions, the 'in2n' method (instruct-NeRF2NeRF) is recommended. The command to train the NERF model with the 'in2n' method would be:\n",
       "\n",
       "```\n",
       "ns-train in2n --data-parser nerfstudio ./fireHydrantDataset\n",
       "```\n",
       "\n",
       "This command will train the NERF model using the 'in2n' method and the nerfstudio data parser. The './fireHydrantDataset' directory is provided as the input data for training.\n",
       "\n",
       "Once the model is trained, you can view the results using the 'ns-viewer' command. The command to load the checkpoint and start the viewer would be:\n",
       "\n",
       "```\n",
       "ns-viewer --load-config ./fireHydrantDataset/checkpoints/last.ckpt\n",
       "```\n",
       "\n",
       "This command will load the checkpoint from the trained model and start the viewer, allowing you to render and interact with the NERF model of the fire hydrant.\n",
       "\n",
       "Please note that the above commands assume that you have the NerfStudio CLI installed and available in your command-line environment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scenario = \"Given a folder of images of a fire hydrant in './fireHydrant', make a NERF model of it and render it. I want to be able to edit this NERF based on text-instructions.\"\n",
    "response = get_completion(prompt.format(context=context, scenario=scenario))\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works fine! However, we are cheating a bit as the scenario is somewhat present verbatim in context. <br/>\n",
    "Scenario: `I want to be able to edit this NERF based on text-instructions` <br/>\n",
    "Context: `instruct-NeRF2NeRF is a method for editing NeRF scenes with text-instructions` <br/>\n",
    "\n",
    "Let's word the scenario a bit differently, like a human would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the API specification, the CLI command to produce a NERF for the given scenario would be as follows:\n",
       "\n",
       "```\n",
       "ns-process-data images --data ./fireHydrant --output-dir ./fireHydrantDataset\n",
       "ns-train nerfacto --data ./fireHydrantDataset\n",
       "ns-viewer --load-config ./fireHydrantDataset/config.txt\n",
       "```\n",
       "\n",
       "Explanation:\n",
       "1. `ns-process-data images --data ./fireHydrant --output-dir ./fireHydrantDataset`: This command processes the images in the `./fireHydrant` folder and generates a nerfstudio dataset in the `./fireHydrantDataset` directory. It automatically scales the images and calculates camera poses using COLMAP.\n",
       "2. `ns-train nerfacto --data ./fireHydrantDataset`: This command trains a NERF model using the `nerfacto` method on the generated dataset in the `./fireHydrantDataset` directory.\n",
       "3. `ns-viewer --load-config ./fireHydrantDataset/config.txt`: This command loads the trained model from the `./fireHydrantDataset` directory and starts the viewer to render and visualize the NERF model.\n",
       "\n",
       "To edit the NERF model, you would need to use additional commands or methods specific to the desired editing functionality. The provided API specification does not include specific details about editing NERF models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scenario = \"Given a folder of images of a fire hydrant in './fireHydrant', make a NERF model of it and render it. I'd like to edit this NERF\"\n",
    "response = get_completion(prompt.format(context=context, scenario=scenario))\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To produce a NERF model for the given scenario and edit it with text, you can use the following CLI command:\n",
       "\n",
       "```\n",
       "ns-process-data images --data ./fireHydrant --output-dir ./fireHydrantDataset\n",
       "```\n",
       "\n",
       "This command will process the images in the './fireHydrant' folder and generate a nerfstudio dataset in the './fireHydrantDataset' directory.\n",
       "\n",
       "After processing the data, you can train a NERF model using the 'ns-train' command. Since you want to edit the NERF with text, you can use the 'in2n' method:\n",
       "\n",
       "```\n",
       "ns-train in2n --data-parser nerfstudio ./fireHydrantDataset\n",
       "```\n",
       "\n",
       "This command will train an 'in2n' NERF model using the 'nerfstudio' data parser and the dataset generated from the previous step.\n",
       "\n",
       "Finally, to view the results and render the edited NERF, you can use the 'ns-viewer' command:\n",
       "\n",
       "```\n",
       "ns-viewer --load-config ./fireHydrantDataset/checkpoints/last.ckpt\n",
       "```\n",
       "\n",
       "This command will load the trained model checkpoint and start the viewer, allowing you to visualize and edit the NERF model.\n",
       "\n",
       "Please note that the above commands assume that you have the NerfStudio CLI installed and available in your system's PATH."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scenario = \"Given a folder of images of a fire hydrant in './fireHydrant', make a NERF model of it and render it. I'd like to edit this NERF with text.\"\n",
    "response = get_completion(prompt.format(context=context, scenario=scenario))\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope again. Let's try some basic prompt engineering. First up, the zero-shot Chain of Thought trick.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To produce a NERF model for the given scenario and enable text editing, the following CLI command can be used:\n",
       "\n",
       "```\n",
       "ns-process-data images --data ./fireHydrant --output-dir ./fireHydrantDataset\n",
       "```\n",
       "This command will process the images in the './fireHydrant' folder and generate a nerfstudio dataset in the './fireHydrantDataset' directory.\n",
       "\n",
       "```\n",
       "ns-train nerfacto --data ./fireHydrantDataset\n",
       "```\n",
       "This command will train a NERF model using the 'nerfacto' method and the generated dataset from the previous step.\n",
       "\n",
       "```\n",
       "ns-viewer --load-config ./fireHydrantDataset/checkpoints/nerfacto\n",
       "```\n",
       "This command will load the trained NERF model and start the viewer, allowing you to render and visualize the results.\n",
       "\n",
       "To edit the NERF model with text, you can use the 'in2n' method. Assuming you have a text instruction file named 'edit_instructions.txt', the following command can be used:\n",
       "\n",
       "```\n",
       "ns-train in2n --data ./fireHydrantDataset --edit-instructions ./edit_instructions.txt\n",
       "```\n",
       "This command will use the 'in2n' method to edit the NERF model based on the instructions provided in the 'edit_instructions.txt' file.\n",
       "\n",
       "Please note that the above commands are just an example and may require additional arguments or modifications based on the specific implementation and requirements of the NerfStudio CLI."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scenario = \"Given a folder of images of a fire hydrant in './fireHydrant', make a NERF model of it and render it. I'd like to edit this NERF with text. Let's think step by step.\"\n",
    "response = get_completion(prompt.format(context=context, scenario=scenario))\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets the model right, but hallucinates `--edit_instructions`. Next up, self-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = f\"\"\"\n",
    "You are a programmer specialized in producing NeRFs. You're given an API specification for the NerfStudio CLI, which can be used to make NERFs for several scenarios. \n",
    "API specification: ```{context}```\n",
    "Based on this API specification, write a CLI command that will produce a NERF for the following scenario: \n",
    "\n",
    "Your task is to determine if the student's solution \\\n",
    "is correct or not.\n",
    "To solve the problem do the following:\n",
    "- First, work out your own solution to the problem. \n",
    "- Then compare your solution to the student's solution \\ \n",
    "and evaluate if the student's solution is correct or not. \n",
    "Don't decide if the student's solution is correct until \n",
    "you have done the problem yourself.\n",
    "\n",
    "Use the following format:\n",
    "Question:\n",
    "```\n",
    "question here\n",
    "```\n",
    "Student's solution:\n",
    "```\n",
    "student's solution here\n",
    "```\n",
    "Actual solution:\n",
    "```\n",
    "steps to work out the solution and your solution here\n",
    "```\n",
    "Is the student's solution the same as actual solution \\\n",
    "just calculated:\n",
    "```\n",
    "yes or no\n",
    "```\n",
    "Student grade:\n",
    "```\n",
    "correct or incorrect\n",
    "```\n",
    "\n",
    "Question:\n",
    "```\n",
    "Given a folder of images of a fire hydrant in './fireHydrant', make a NERF model of it and render it. I'd like to edit this NERF with text.\n",
    "``` \n",
    "Student's solution:\n",
    "```\n",
    "'To produce a NERF model for the given scenario and enable text editing, you can use the following CLI command:\\n\\n```\\nns-process-data images --data ./fireHydrant --output-dir ./fireHydrantDataset\\nns-train nerfacto --data ./fireHydrantDataset\\nns-viewer --load-config ./fireHydrantDataset/config.txt\\n```\\n\\nExplanation:\\n1. `ns-process-data images --data ./fireHydrant --output-dir ./fireHydrantDataset`: This command processes the images in the `./fireHydrant` folder and generates a nerfstudio dataset in the `./fireHydrantDataset` directory. It automatically scales the images and calculates camera poses using COLMAP.\\n2. `ns-train nerfacto --data ./fireHydrantDataset`: This command trains a NERF model using the `nerfacto` method on the generated dataset in the `./fireHydrantDataset` directory.\\n3. `ns-viewer --load-config ./fireHydrantDataset/config.txt`: This command loads the trained model checkpoint from the `./fireHydrantDataset` directory and starts the viewer, allowing you to render and edit the NERF model.\\n\\nNote: Make sure you have the NerfStudio CLI installed and accessible in your environment before running these commands.'\n",
    "```\n",
    "Actual solution:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To produce a NERF model for the given scenario and enable text editing, you can use the following CLI command:\n",
       "\n",
       "```\n",
       "ns-process-data images --data ./fireHydrant --output-dir ./fireHydrantDataset\n",
       "ns-train in2n --data ./fireHydrantDataset\n",
       "ns-viewer --load-config ./fireHydrantDataset/config.txt\n",
       "```\n",
       "\n",
       "Explanation:\n",
       "1. `ns-process-data images --data ./fireHydrant --output-dir ./fireHydrantDataset`: This command processes the images in the `./fireHydrant` folder and generates a nerfstudio dataset in the `./fireHydrantDataset` directory. It automatically scales the images and calculates camera poses using COLMAP.\n",
       "2. `ns-train in2n --data ./fireHydrantDataset`: This command trains a NERF model using the `in2n` method on the generated dataset in the `./fireHydrantDataset` directory. The `in2n` method allows for editing the NERF model with text instructions.\n",
       "3. `ns-viewer --load-config ./fireHydrantDataset/config.txt`: This command loads the trained model checkpoint from the `./fireHydrantDataset` directory and starts the viewer, allowing you to render and edit the NERF model.\n",
       "\n",
       "Note: Make sure you have the NerfStudio CLI installed and accessible in your environment before running these commands.\n",
       "\n",
       "Is the student's solution the same as the actual solution just calculated:\n",
       "No\n",
       "\n",
       "Student grade:\n",
       "Incorrect"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion(prompt2)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepwise, Sequential Prompting with Manual Intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a sequence of steps that build atop each other. I'd like to be able to intervene or ask clarifying questions when I feek there is an obvious mistake with a step. Since the model is capable of self-criticism, we can try something else. <br/>\n",
    "\n",
    "Let's leverage this self-criticism ability to mock an interactive session where a user can reject a response at a step and/or provide feedback to request a correction.\n",
    "- A command is generated at each step. I can either accept it by pressing 'enter' or reject it and let the model know what's wrong\n",
    "- For eg, in the example below, we ask for an editable model but the model suggests nerfacto, which is not directly editable. <br/>\n",
    "So I reject the suggestion and say that the 'nerfacto may not be editable'. This self-critiques and updates the step to use in2n instead.<br/>\n",
    "- Such a flow can be quite useful when dealing with chains of commands or semi-technical users trying to reduce some manual effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction Flow:\n",
      "++++++++++++++++++++\n",
      "The following CLI modules is available for use:\n",
      "- ns-process-data: Process images into a nerfstudio dataset. Scales images to a specified size and calculates the camera poses for each image using COLMAP.\n",
      "  usage: ns-process-data [-h]\n",
      "                       {images,video,polycam,metashape,realitycapture,record3d}\n",
      "  ns-process-data images [-h] --data PATH --output-dir PATH\n",
      "                       [--verbose | --no-verbose]\n",
      "                       [--camera-type {perspective,fisheye,equirectangular}]\n",
      "                       [--matching-method {exhaustive,sequential,vocab_tree}]\n",
      "                       [--sfm-tool {any,colmap,hloc}]\n",
      "                       [--refine-pixsfm | --no-refine-pixsfm]\n",
      "                       [--feature-type \n",
      "{any,sift,superpoint,superpoint_aachen,superpoint_max,superpoint_inloc,r2d2,d2\n",
      "net-ss,sosnet,disk}]\n",
      "                       [--matcher-type \n",
      "{any,NN,superglue,superglue-fast,NN-superpoint,NN-ratio,NN-mutual,adalam}]\n",
      "                       [--num-downscales INT]\n",
      "                       [--skip-colmap | --no-skip-colmap]\n",
      "                       [--skip-image-processing | --no-skip-image-processing]\n",
      "                       [--colmap-model-path PATH] [--colmap-cmd STR]\n",
      "                       [--images-per-equirect {8,14}]\n",
      "                       [--crop-factor FLOAT FLOAT FLOAT FLOAT]\n",
      "                       [--crop-bottom FLOAT] [--gpu | --no-gpu]\n",
      "                       [--use-sfm-depth | --no-use-sfm-depth]\n",
      "                       [--include-depth-debug | --no-include-depth-debug]\n",
      "\n",
      "- ns-train: Primary interface for training a NeRF model\n",
      "  usage: ns-train {method} [method args] {dataparser} [dataparser args]\n",
      "    \n",
      "    To learn about the available methods:\n",
      "    ns-train --help\n",
      "    To learn about a methods parameters:\n",
      "\n",
      "    ns-train {method} --help\n",
      "    By default the nerfstudio dataparser is used. If you would like to use a different dataparser it can be specified after all of the method arguments. For a list of dataparser options:\n",
      "    ns-train {method} {dataparser} --help\n",
      "\n",
      "    methods:\n",
      "        - vanilla-nerf: The original NERF implementation. Generally not recommended for most applications.\n",
      "        - nerfacto: We created the nerfacto model to act as our default for real data captures of static scenes. The model is not existing published work, but rather a combination of many published methods that we have found work well for real data.\n",
      "        - instant-ngp: Many of the main contributions of Instant-NGP are built into our Nerfacto method, so for real-world scenes, we recommend using the Nerfacto model.\n",
      "        - in2n: instruct-NeRF2NeRF is a method for editing NeRF scenes with text-instructions. Given a NeRF of a scene and the collection of images used to reconstruct it, the method uses an image-conditioned diffusion model (InstructPix2Pix) to iteratively edit the input images while optimizing the underlying scene, resulting in an optimized 3D scene that respects the edit instruction. The paper demonstrates that their method is able to edit large-scale, real-world scenes, and is able to accomplish more realistic, targeted edits than prior work.\n",
      "        - lerf: LERF enables pixel-aligned queries of the distilled 3D CLIP embeddings without relying on region proposals, masks, or fine-tuning, supporting long-tail open-vocabulary queries hierarchically across the volume.\n",
      "        - tetra-nerf: The input to Tetra-NeRF is a point cloud which is triangulated to get a set of tetrahedra used to represent the radiance field. Rays are sampled, and the field is queried. The barycentric interpolation is used to interpolate tetrahedra vertices, and the resulting features are passed to a shallow MLP to get the density and colours for volumetric rendering.\n",
      "\n",
      "- ns-viewer: Load a checkpoint and start the viewer.\n",
      "  usage: ns-viewer [-h] --load-config PATH [--viewer.relative-log-filename STR]\n",
      "    [--viewer.websocket-port {None}|INT]\n",
      "    [--viewer.websocket-port-default INT]\n",
      "    [--viewer.websocket-host STR]\n",
      "    [--viewer.max-num-display-images INT]\n",
      "    [--viewer.quit-on-train-completion | \n",
      "    --viewer.no-quit-on-train-completion]\n",
      "                [--viewer.image-format {jpeg,png}]\n",
      "                [--viewer.jpeg-quality INT] [--vis {viewer,viewer_beta}]\n",
      "\n",
      "Given a folder of images of a fire hydrant in './fireHydrant', make a NERF model of it and render it. I'd like to edit this NERF with text.\n",
      "\n",
      "\n",
      "Step 1: Process your images into a nerfstudio dataset using ns-process-data. Give the actual command\n",
      "\n",
      "The command to process the images into a nerfstudio dataset using ns-process-data would be:\n",
      "\n",
      "ns-process-data images --data ./fireHydrant --output-dir ./output --camera-type perspective --sfm-tool colmap --feature-type superpoint --matcher-type superglue --num-downscales 4 --skip-colmap --gpu --use-sfm-depth\n",
      "\n",
      "Step 2: Train the model using ns-train. Give the actual command\n",
      "\n",
      "Step 2: Edit the NERF model using the \"in2n\" method and the nerfstudio dataparser. Give the actual command.\n",
      "\n",
      "The command to edit the NERF model using the \"in2n\" method and the nerfstudio dataparser would be:\n",
      "\n",
      "ns-train in2n --data ./output\n",
      "\n",
      "Replace \"in2n\" with the desired method and add any additional arguments specific to that method.\n",
      "\n",
      "Step 3: Visualise the results with ns-viewer. Give the actual command\n",
      "\n",
      "The command to visualize the results with ns-viewer would be:\n",
      "\n",
      "ns-viewer --load-config ./output/config.txt --viewer.relative-log-filename ./output/nerf.log --viewer.websocket-port 8000 --viewer.max-num-display-images 10 --viewer.quit-on-train-completion --viewer.image-format jpeg --viewer.jpeg-quality 90 --vis viewer\n",
      "\n",
      "\\Final resulting responses:\n",
      "++++++++++++++++++++\n",
      "Step 1: The command to process the images into a nerfstudio dataset using ns-process-data would be:\n",
      "\n",
      "ns-process-data images --data ./fireHydrant --output-dir ./output --camera-type perspective --sfm-tool colmap --feature-type superpoint --matcher-type superglue --num-downscales 4 --skip-colmap --gpu --use-sfm-depth\n",
      "Step 2: Step 2: Edit the NERF model using the \"in2n\" method and the nerfstudio dataparser. Give the actual command.\n",
      "\n",
      "The command to edit the NERF model using the \"in2n\" method and the nerfstudio dataparser would be:\n",
      "\n",
      "ns-train in2n --data ./output\n",
      "\n",
      "Replace \"in2n\" with the desired method and add any additional arguments specific to that method.\n",
      "Step 3: The command to visualize the results with ns-viewer would be:\n",
      "\n",
      "ns-viewer --load-config ./output/config.txt --viewer.relative-log-filename ./output/nerf.log --viewer.websocket-port 8000 --viewer.max-num-display-images 10 --viewer.quit-on-train-completion --viewer.image-format jpeg --viewer.jpeg-quality 90 --vis viewer\n"
     ]
    }
   ],
   "source": [
    "# Function to get user input for the next prompt\n",
    "def get_user_input(prompt_num):\n",
    "    user_input = input(f\"Step {prompt_num} - Enter to accept or type 'r' to reject: \")\n",
    "    return user_input.strip()\n",
    "\n",
    "def main():\n",
    "    initial_scenario = \"Given a folder of images of a fire hydrant in './fireHydrant', make a NERF model of it and render it. I'd like to edit this NERF with text.\\n\"\n",
    "    step_prompts = [\n",
    "        \"Step 1: Process your images into a nerfstudio dataset using ns-process-data. Give the actual command\\n\",\n",
    "        \"Step 2: Train the model using ns-train. Give the actual command\\n\",\n",
    "        \"Step 3: Visualise the results with ns-viewer. Give the actual command\\n\",\n",
    "    ]\n",
    "\n",
    "    with open('context_noWorflow.txt', 'r') as file:\n",
    "        context2 = file.read()\n",
    "    context2 += '\\n\\n'+initial_scenario + '\\n'\n",
    "    responses = []\n",
    "    \n",
    "\n",
    "    for i, prompt in enumerate(step_prompts, start=1):\n",
    "        context2 += '\\n'+ prompt+ '\\n'\n",
    "        prompt_text =  context2\n",
    "        display(Markdown(prompt_text))\n",
    "        response = get_completion(prompt_text)\n",
    "        display(Markdown(response))\n",
    "        prompt_text += response + \"\\n\"\n",
    "        mistake = get_user_input(i)\n",
    "\n",
    "        if mistake.lower() == \"r\":\n",
    "            mistake = input(\"What was wrong?:\")\n",
    "            prompt_text += mistake + \"\\n\"\n",
    "            prompt_text += \"Correct the last step based on the above feedback. Think step by step and keep it concise.\\n\"\n",
    "            response = get_completion(prompt_text)\n",
    "            display(Markdown(response))\n",
    "        context2 += response + \"\\n\"\n",
    "        responses.append(response)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(\"Interaction Flow:\")\n",
    "    print('+'*20)\n",
    "    print(context2)\n",
    "\n",
    "    print(\"\\Final resulting responses:\")\n",
    "    print('+'*20)\n",
    "    for i, response in enumerate(responses, start=1):\n",
    "        print(f\"Step {i}:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
